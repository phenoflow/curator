services:
  
  llm:
    image: llamacpp-server
    command: --server --host 0.0.0.0 --model /model/ggml-model-Q4_K_M.gguf -c 2048
    ports:
      - '8080:8080'
    volumes: 
      - ${MODEL_PATH}:/model
    restart: unless-stopped
